---
layout:     post
title:      ç®€æ˜“æœç´¢å¼•æ“ï¼ˆä¸€ï¼‰
subtitle:   Pythonç®€æ˜“æœç´¢å¼•æ“åŸç†åŠå®ç°ï¼ˆä¸€ï¼‰å»ºç«‹å€’æ’ç´¢å¼•
date:       2021-01-03
author:     CY
header-img: img/post-bg-food3.jpg
catalog: 	 true
mathjax:       true
tags:
    - çˆªå·´è™«
    - jieba
	- æœç´¢å¼•æ“
---



# Pythonç®€æ˜“æœç´¢å¼•æ“åŸç†åŠå®ç°ï¼ˆä¸€ï¼‰å»ºç«‹å€’æ’ç´¢å¼•

## ä¸€ã€ä»€ä¹ˆæ˜¯å€’æ’ç´¢å¼•

åœ¨**æœç´¢å¼•æ“**æ¯ä¸ªæ–‡ä»¶éƒ½å¯¹åº”ä¸€ä¸ªæ–‡ä»¶IDï¼Œæ–‡ä»¶å†…å®¹è¢«è¡¨ç¤ºä¸ºä¸€ç³»åˆ—å…³é”®è¯çš„é›†åˆï¼ˆå®é™…ä¸Šåœ¨æœç´¢å¼•æ“ç´¢å¼•åº“ä¸­ï¼Œå…³é”®è¯ä¹Ÿå·²ç»è½¬æ¢ä¸ºå…³é”®è¯IDï¼‰ï¼Œä¾‹å¦‚ã€æ–‡æ¡£1ã€‘ç»è¿‡åˆ†è¯ï¼Œæå–äº†20ä¸ªå…³é”®è¯ï¼Œæ¯ä¸ªå…³é”®è¯éƒ½ä¼šè®°å½•ä»–åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°æ¬¡æ•°å’Œå‡ºç°ä½ç½®ã€‚

å¾—åˆ°**æ­£å‘ç´¢å¼•**çš„ç»“æ„å¦‚ä¸‹ï¼š

â€œæ–‡æ¡£1â€çš„ID > å•è¯1ï¼šå‡ºç°æ¬¡æ•°ï¼Œå‡ºç°ä½ç½®åˆ—è¡¨ï¼›å•è¯2ï¼šå‡ºç°æ¬¡æ•°ï¼Œå‡ºç°ä½ç½®åˆ—è¡¨ï¼›â€¦â€¦
â€œæ–‡æ¡£2â€çš„ID > æ­¤æ–‡æ¡£å‡ºç°çš„å…³é”®è¯åˆ—è¡¨ã€‚

â€¦â€¦Â ![image-20210331192704697](https://tva1.sinaimg.cn/large/008eGmZEly1gp3b99k9i6j30wi0egqbs.jpg)

ä¸€èˆ¬æ˜¯é€šè¿‡keyæ‰¾value

#### å€’æ’ç´¢å¼•æ¦‚å¿µ

- æ–‡æ¡£(Document)ï¼šä¸€èˆ¬æœç´¢å¼•æ“çš„å¤„ç†å¯¹è±¡æ˜¯äº’è”ç½‘ç½‘é¡µï¼Œè€Œæ–‡æ¡£è¿™ä¸ªæ¦‚å¿µè¦æ›´å®½æ³›äº›ï¼Œä»£è¡¨ä»¥æ–‡æœ¬å½¢å¼å­˜åœ¨çš„å­˜å‚¨å¯¹è±¡ï¼Œç›¸æ¯”ç½‘é¡µæ¥è¯´ï¼Œæ¶µç›–æ›´å¤šç§å½¢å¼ï¼Œæ¯”å¦‚Wordï¼ŒPDFï¼Œhtmlï¼ŒXMLç­‰ä¸åŒæ ¼å¼çš„æ–‡ä»¶éƒ½å¯ä»¥ç§°ä¹‹ä¸ºæ–‡æ¡£ã€‚å†æ¯”å¦‚ä¸€å°é‚®ä»¶ï¼Œä¸€æ¡çŸ­ä¿¡ï¼Œä¸€æ¡å¾®åšä¹Ÿå¯ä»¥ç§°ä¹‹ä¸ºæ–‡æ¡£ã€‚æˆ‘ä»¬ä½¿ç”¨æ–‡æ¡£æ¥è¡¨å¾æ–‡æœ¬ä¿¡æ¯ã€‚

- æ–‡æ¡£é›†åˆ(Document Collection)ï¼šç”±è‹¥å¹²æ–‡æ¡£æ„æˆçš„é›†åˆç§°ä¹‹ä¸ºæ–‡æ¡£é›†åˆã€‚æ¯”å¦‚æµ·é‡çš„äº’è”ç½‘ç½‘é¡µæˆ–è€…è¯´å¤§é‡çš„ç”µå­é‚®ä»¶éƒ½æ˜¯æ–‡æ¡£é›†åˆçš„å…·ä½“ä¾‹å­ã€‚
  æ–‡æ¡£ç¼–å·(Document ID)ï¼šåœ¨æœç´¢å¼•æ“å†…éƒ¨ï¼Œä¼šå°†æ–‡æ¡£é›†åˆå†…æ¯ä¸ªæ–‡æ¡£èµ‹äºˆä¸€ä¸ªå”¯ä¸€çš„å†…éƒ¨ç¼–å·ï¼Œä»¥æ­¤ç¼–å·æ¥ä½œä¸ºè¿™ä¸ªæ–‡æ¡£çš„å”¯ä¸€æ ‡è¯†ï¼Œè¿™æ ·æ–¹ä¾¿å†…éƒ¨å¤„ç†ï¼Œæ¯ä¸ªæ–‡æ¡£çš„å†…éƒ¨ç¼–å·å³ç§°ä¹‹ä¸ºâ€œæ–‡æ¡£ç¼–å·â€ï¼Œç”¨DocIDæ¥ä¾¿æ·åœ°ä»£è¡¨æ–‡æ¡£ç¼–å·ã€‚

- å•è¯ç¼–å·(Word ID)ï¼šä¸æ–‡æ¡£ç¼–å·ç±»ä¼¼ï¼Œæœç´¢å¼•æ“å†…éƒ¨ä»¥å”¯ä¸€çš„ç¼–å·æ¥è¡¨å¾æŸä¸ªå•è¯ï¼Œå•è¯ç¼–å·å¯ä»¥ä½œä¸ºæŸä¸ªå•è¯çš„å”¯ä¸€è¡¨å¾ã€‚

- å€’æ’ç´¢å¼•(Inverted Index)ï¼šå€’æ’ç´¢å¼•æ˜¯å®ç°â€œå•è¯-æ–‡æ¡£çŸ©é˜µâ€çš„ä¸€ç§å…·ä½“å­˜å‚¨å½¢å¼ï¼Œé€šè¿‡å€’æ’ç´¢å¼•ï¼Œå¯ä»¥æ ¹æ®å•è¯å¿«é€Ÿè·å–åŒ…å«è¿™ä¸ªå•è¯çš„æ–‡æ¡£åˆ—è¡¨ã€‚å€’æ’ç´¢å¼•ä¸»è¦ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼šâ€œå•è¯è¯å…¸â€å’Œâ€œå€’æ’æ–‡ä»¶â€ã€‚

- å•è¯è¯å…¸(Lexicon)ï¼šæœç´¢å¼•æ“çš„é€šå¸¸ç´¢å¼•å•ä½æ˜¯å•è¯ï¼Œå•è¯è¯å…¸æ˜¯ç”±æ–‡æ¡£é›†åˆä¸­å‡ºç°è¿‡çš„æ‰€æœ‰å•è¯æ„æˆçš„å­—ç¬¦ä¸²é›†åˆï¼Œå•è¯è¯å…¸å†…æ¯æ¡ç´¢å¼•é¡¹è®°è½½å•è¯æœ¬èº«çš„ä¸€äº›ä¿¡æ¯ä»¥åŠæŒ‡å‘â€œå€’æ’åˆ—è¡¨â€çš„æŒ‡é’ˆã€‚

- å€’æ’åˆ—è¡¨(PostingList)ï¼šå€’æ’åˆ—è¡¨è®°è½½äº†å‡ºç°è¿‡æŸä¸ªå•è¯çš„æ‰€æœ‰æ–‡æ¡£çš„æ–‡æ¡£åˆ—è¡¨åŠå•è¯åœ¨è¯¥æ–‡æ¡£ä¸­å‡ºç°çš„ä½ç½®ä¿¡æ¯ï¼Œæ¯æ¡è®°å½•ç§°ä¸ºä¸€ä¸ªå€’æ’é¡¹(Posting)ã€‚æ ¹æ®å€’æ’åˆ—è¡¨ï¼Œå³å¯è·çŸ¥å“ªäº›æ–‡æ¡£åŒ…å«æŸä¸ªå•è¯ã€‚

- å€’æ’æ–‡ä»¶(Inverted File)ï¼šæ‰€æœ‰å•è¯çš„å€’æ’åˆ—è¡¨å¾€å¾€é¡ºåºåœ°å­˜å‚¨åœ¨ç£ç›˜çš„æŸä¸ªæ–‡ä»¶é‡Œï¼Œè¿™ä¸ªæ–‡ä»¶å³è¢«ç§°ä¹‹ä¸ºå€’æ’æ–‡ä»¶ï¼Œå€’æ’æ–‡ä»¶æ˜¯å­˜å‚¨å€’æ’ç´¢å¼•çš„ç‰©ç†æ–‡ä»¶ã€‚

## äºŒã€å®ç°æ­¥éª¤

  1ï¼‰é€‰å®šæ•°æ®æºï¼Œå¦‚æŸç±»ç½‘ç«™ã€æŸç±»æœŸåˆŠã€æŸç±»ä¼šè®®ã€‚
  2ï¼‰ï¼ˆæ‰‹å·¥/è‡ªåŠ¨ï¼‰è·å–æ•°æ®æºä¸­çš„æ–‡æœ¬ä¿¡æ¯ï¼Œå¦‚å°†æ¯ä¸ªç½‘é¡µä½œä¸ºä¸€ç¯‡æ–‡çŒ®ï¼Œå­˜ä¸º.txt æ–‡æ¡£ã€‚
  3ï¼‰åˆ†è¯ç®—æ³•ï¼šä¸­æ–‡æ–‡æ¡£ï¼Œé€‰ç”¨ä¸­æ–‡åˆ†è¯å·¥å…·æ¥å®ç°ã€‚
  4ï¼‰æ’åºç®—æ³•ï¼šå¯¹æå–çš„æ‰€æœ‰ items(ä¿¡æ¯é¡¹)è¿›è¡Œæ’åºã€‚
  5ï¼‰è¯é¢‘ç®—æ³•ï¼šç»Ÿè®¡åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¯ä¸ª item çš„è¯é¢‘ tfã€‚
  6ï¼‰å»é‡ç®—æ³•ï¼šè®¡ç®—å‡ºç°æ¯ä¸ª item çš„æ–‡æ¡£ä¸ªæ•° dfï¼Œå°†é‡å¤å‡ºç°çš„ item è¿›è¡Œå»é‡å¤„ç†ã€‚
  7ï¼‰åˆ›å»ºç´¢å¼•ç»“æ„ï¼šå»ºç«‹å­—å…¸ç»“æ„å’Œ PostingList ç»“æ„ï¼Œå­˜å‚¨ items å’Œ dfã€DocIDs å’Œ tf

## ä¸‰ã€ä»£ç å®ç°

### 1.æ•°æ®çˆ¬å–ï¼ˆè±†ç“£Top250ï¼‰

```python
import requests
from bs4 import BeautifulSoup
import time
import os
import sys

def request_douban(url): # è¯·æ±‚å¹¶è·å–è±†ç“£çš„æºç 
    # headers = {''}
    # cookies = {''}
    # cyheaders = {''}
    # cycookies = {''}
    try:
        #response = requests.get(url, headers=cyheaders, cookies=cycookies)
        response = requests.get(url) #
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None

n = 226


def saveToEveryFile(soup, *parm):
    if soup.find(class_= 'all hidden') is not None:
        intro = soup.find(class_='all hidden').get_text().replace(u' ','')
    else:
        intro = soup.find(property='v:summary').get_text().replace(u' ','')
    item_director = soup.find(text='å¯¼æ¼”').parent.parent.find(class_='attrs').get_text().strip()
    itemkind = ''
    for i in soup.find_all(property='v:genre'):
        itemkind = itemkind + ' ' + i.get_text().strip()
    myPath = os.path.join(os.getcwd(), "exp" + os.sep + "data" + os.sep + "movie" + str(n) + ".txt")  # windowsä¸‹å»æ‰exp + os.sep +
    myfile = open(myPath, "w", encoding='utf-8')
    for item in parm:
        myfile.write(item)
    myfile.write("å¯¼æ¼”:" + item_director + "\n")
    myfile.write('ç±»å‹:' + itemkind + '\n')
    myfile.write(intro+"\n")


def saveToFile(soup):
    myfile = open("douban.txt", "a+", encoding='utf-8')
    list = soup.find(class_='grid_view').find_all('li')
    for item in list:
        item_url = item.find('a').get('href')
        item_name = item.find(class_='title').string
        item_img = item.find('a').find('img').get('src')
        item_index = item.find(class_='').string
        item_score = item.find(class_='rating_num').string
        item_author = item.find('p').text
        if (item.find(class_='inq') != None):
            item_intr = item.find(class_='inq').string

        # print('çˆ¬å–ç”µå½±ï¼š' + item_index + ' | ' + item_name +' | ' + item_img +' | ' + item_score +' | ' + item_author +' | ' + item_intr )
#        print('çˆ¬å–ç”µå½±ï¼š' + item_index + ' | ' + item_name + ' | ' + item_score + ' | ' + item_intr)

        global n

        pageHtml = request_douban(item_url)
        pageSoup = BeautifulSoup(pageHtml, 'lxml')
        # print(pageSoup)
        saveToEveryFile(pageSoup, "å½±ç‰‡åç§°:" + item_name.strip(), "å½±ç‰‡åºå·:" + item_index.strip(),"è±†ç“£åˆ†æ•°:" + item_score.strip())
        myfile.write(item_url + " | " + item_name + ":" + item_intr + "\n")
        n = n + 1
        time.sleep(5)

def main(page):
    url = 'https://movie.douban.com/top250?start=' + str(page * 25) + '&filter='
    html = request_douban(url)
    soup = BeautifulSoup(html, 'lxml')
    saveToFile(soup)

if __name__ == '__main__':
    for i in range(9, 10):
        main(i)

```

### 2. å•è¯åˆ†è¯

ä¸­æ–‡åˆ†è¯ï¼Œä½¿ç”¨[**jieba**](https://github.com/fxsjy/jieba)åˆ†è¯å·¥å…·è¿›è¡Œå•è¯åˆ†è¯ã€‚ä¸‹é¢ç®€å•ä»‹ç»ä¸€ä¸‹**jieba**

##### jieba

![image-20210401095110590](https://tva1.sinaimg.cn/large/008eGmZEly1gp408c0rlcj315o08wmzi.jpg)

jiebaåº“æ˜¯ä¸€æ¬¾ä¼˜ç§€çš„ Python ç¬¬ä¸‰æ–¹ä¸­æ–‡åˆ†è¯åº“ï¼Œjiebaåº“ä¸­ç”¨äºåˆ†è¯çš„æ–¹æ³•æœ‰ä¸‰ä¸ªï¼š`jieba.cut` `jieba.cut_for_search` `jieba.lacut`

ğŸŒ²`jieba.cut`ï¼šç»™å®šä¸­æ–‡å­—ç¬¦ä¸²ï¼Œåˆ†è§£åè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œéœ€è¦forå¾ªç¯è®¿é—®ï¼Œæœ‰å››ä¸ªå‚æ•°ï¼š

- éœ€è¦åˆ†è¯çš„å­—ç¬¦ä¸²
- *cut_all* ï¼šæ§åˆ¶æ˜¯å¦é‡‡ç”¨å…¨æ¨¡å¼
- *HMM*ï¼šæ˜¯å¦ä½¿ç”¨HMMæ¨¡å‹ï¼ˆé»˜è®¤Trueï¼‰
- *use_paddle*ï¼šå‚æ•°ç”¨æ¥æ§åˆ¶æ˜¯å¦ä½¿ç”¨paddleæ¨¡å¼ä¸‹çš„åˆ†è¯æ¨¡å¼ï¼Œpaddleæ¨¡å¼é‡‡ç”¨å»¶è¿ŸåŠ è½½æ–¹å¼ï¼Œé€šè¿‡enable_paddleæ¥å£å®‰è£…paddlepaddle-tinyï¼Œå¹¶ä¸”importç›¸å…³ä»£ç 

ğŸŒ²`jieba.cut_for_search`ï¼šè¯¥æ–¹æ³•å’Œcutä¸€æ ·ï¼Œåˆ†è§£åè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œéœ€è¦ç”¨forå¾ªç¯è®¿é—®ã€‚ä¸è¿‡å®ƒæ˜¯æœç´¢å¼•æ“æ¨¡å¼ï¼Œåœ¨ç²¾ç¡®æ¨¡å¼çš„åŸºç¡€ä¸Šï¼Œå¯¹é•¿è¯å†æ¬¡åˆ‡åˆ†ï¼Œæé«˜å¬å›ç‡ï¼Œé€‚åˆç”¨äºæœç´¢å¼•æ“åˆ†è¯ã€‚

ğŸŒ²`jieba.lcut`ï¼šå’Œ`jieba.cut`ä½¿ç”¨æ–¹æ³•ä¸€æ ·ï¼Œä¸è¿‡è¿”å›çš„æ˜¯åˆ—è¡¨listã€‚

jieba æ”¯æŒä¸‰ç§åˆ†è¯æ¨¡å¼ï¼šç²¾ç¡®æ¨¡å¼ã€å…¨æ¨¡å¼å’Œæœç´¢å¼•æ“æ¨¡å¼ï¼Œä¸‹é¢æ˜¯ä¸‰ç§æ¨¡å¼çš„ç‰¹ç‚¹ã€‚

- ç²¾ç¡®æ¨¡å¼ï¼šè¯•å›¾å°†è¯­å¥æœ€ç²¾ç¡®çš„åˆ‡åˆ†ï¼Œä¸å­˜åœ¨å†—ä½™æ•°æ®ï¼Œé€‚åˆåšæ–‡æœ¬åˆ†æ

- å…¨æ¨¡å¼ï¼šå°†è¯­å¥ä¸­æ‰€æœ‰å¯èƒ½æ˜¯è¯çš„è¯è¯­éƒ½åˆ‡åˆ†å‡ºæ¥ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä½†æ˜¯å­˜åœ¨å†—ä½™æ•°æ®

- æœç´¢å¼•æ“æ¨¡å¼ï¼šåœ¨ç²¾ç¡®æ¨¡å¼çš„åŸºç¡€ä¸Šï¼Œå¯¹é•¿è¯å†æ¬¡è¿›è¡Œåˆ‡åˆ†

### 3. å»ºç«‹å€’æ’ç´¢å¼•

å°†çˆ¬å–çš„æ•°æ®è¿›è¡Œåˆ†è¯è®¡ç®—*tf, df*ï¼Œå»ºç«‹å­—å…¸ç»“æ„å’Œ*PostingList*å­˜å‚¨è¯æ¡å*df*ä»¥åŠ*df*ä¸ª*docID*å’Œ*tf.* 

1ï¸âƒ£ æˆ‘ä»¬éœ€è¦åˆ›å»ºå¦‚å›¾æ‰€ç¤ºçš„ç»“æ„ï¼š

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp40ilzjsjj30vg0j6159.jpg" alt="image-20210401100104090" style="zoom:50%;" />

2âƒ£ï¸å¯¹æå–çš„itemï¼ˆä¿¡æ¯é¡¹è¿›è¡Œæ’åºï¼‰

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp40tofatzj30vg0iogq5.jpg" alt="image-20210401101142672" style="zoom:50%;" />

3âƒ£ï¸ ç»Ÿè®¡åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¯ä¸ª item çš„è¯é¢‘ tf

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp40u9yk6tj30vq0ieage.jpg" alt="image-20210401101216875" style="zoom:50%;" />

4âƒ£ï¸ è®¡ç®—å‡ºç°æ¯ä¸ª item çš„æ–‡æ¡£ä¸ªæ•° dfï¼Œå°†é‡å¤å‡ºç°çš„ item è¿›è¡Œå»é‡å¤„ç†

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp40vk95olj30va0nw10o.jpg" alt="image-20210401101330090" style="zoom:50%;" />

5âƒ£ï¸ å€’æ’ç´¢å¼•å»ºç«‹å®Œæˆï¼Œè¾“å‡ºå­—å…¸ç»“æ„å’ŒPosting Listç»“æ„åˆ°æ–‡ä»¶

æœ€ç»ˆå½¢æˆçš„å€’æ’ç´¢å¼•ç»“æ„å¦‚å›¾ï¼š

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20210401101538740.png" alt="image-20210401101538740" style="zoom:50%;" />

![image-20210401103146402](https://tva1.sinaimg.cn/large/008eGmZEly1gp41ejtpufj30fo070t9b.jpg)

### 4. å…·ä½“ä»£ç 

```python
def segmentations(ID):
    moviePath = NAME + str(ID) + '.txt'
    singleFile = open(os.path.join(dataPath, moviePath), 'r', encoding='utf-8')
    contents = singleFile.read()
    segmentation = jieba.cut_for_search(contents)
    
    cnt = 0
    itemdict = dict()
    for items in segmentation:
        if items not in rubbish and items is not None:
            # sortedList.append(items)
            cnt = cnt + 1
            if itemdict.get(items) is None:
                itemdict.update({items: 0})
            itemdict[items] = itemdict[items] + 1 # è®°å½•æ¯ä¸ªitemå‡ºç°çš„æ¬¡æ•°
    for item, count in itemdict.items():
        tfdict[item] = 1.0 * count / cnt # è®¡ç®—tfï¼šå•è¯é¢‘ç‡ä¿¡æ¯
        resultdict[item].update({'df': resultdict.get(item).get('df') + 1}) # è®¡ç®—dfï¼šæ–‡æ¡£é¢‘ç‡ä¿¡æ¯
        resultdict.get(item).update({ID: tfdict[item]})
```

```python
if __name__ == '__main__':
    n = 1
    while n <= 250:
        segmentations(n)
        print(n)
        n = n + 1
    resultfile = open(os.path.join(dataPath, 'result.txt'), 'w', encoding='utf-8')
    for items, val in resultdict.items():
        resultfile.write(items + ' ' + 'df:' + str(val.get('df')) + '\n')
        for innerItem, innerVal in val.items():
            if innerItem is not 'df':
                innerItem = 'DocID' + str(innerItem)
                innerVal = 'tf: ' + str(innerVal)
            else:
                continue
            resultfile.write(str(innerItem) + ' ' + str(innerVal) + '\n')
```

