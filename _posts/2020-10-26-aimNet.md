---
layout:     post
title:      Federated Learning for Vision-and-Language Grounding Problems阅读笔记
subtitle:   这篇论文提出了一种联邦学习框架，可以从不同的任务中获得各种类型的图像表示，然后将它们融合在一起以形成细粒度的图像表示
date:       2020-10-26
author:     CY
header-img: img/post-bg-food3.jpg
catalog: 	 true
mathjax:       true
tags:
    - Federated Learning
    - Multi-Task
---

### 论文主要工作

- 提出一种联邦学习框架，通过生成细粒度的图像表示，框架提高了在不需要共享下游任务数据的情况下vision-and-language grounding问题的性能
- 设计对齐、集成和映射网络【Aligning, Integrating and Mapping Net work (aimNet)】，在框架中实现了分布式模型，有效 自动地并将从图像提取出的视觉和文本特征转换成细粒度图像表征。
- 在三个联邦学习设置上进行了验证

下面对这篇论文做个详细的介绍

### 背景知识

首先先做一些背景介绍，什么是 vision-and-language？
我们知道在早些年，Computer Vision（计算机视觉）和 Natural Language Processing （自然语言处理）一直是两个独立的研究方向。他们都需要用到很多机器学习，模式识别等技术，同时，他们也都受益于近几年的深度神经网络的进步，可以说这两个领域目前的 state-of-art，都是基于神经网络的，而且很多任务，比如 CV 里的物体识别检测，NLP 里的机器翻译，都已经达到了可以实用的程度。于是从 2015 年开始，有一个趋势就是将视觉与语言进行一定程度的结合，从而产生出一些新的应用与挑战。比如image captioning，visual question answering 等比较经典的 vision-and-language 任务。

那在这篇论文里面，就是以image caption和VQA两个具有代表性的任务做实验的。 那我们先来看一下这两个任务分别是在做什么？

#### image caption

image caption是指图像描述，早些年，大概15年左右，这个方向的主流模型是基于 CNN-RNN 框架的，即输入一张图像，先用一个 pre-trained 的 CNN 去提取图像特征，然后，将这些 CNN 特征输入到 RNN，也就是递归神经网络当中去生成单词序列。这种模型表面上看起来非常吸引人，依赖于强大的深度神经网络，能够用 end-to-end 的方式学习到一个从图像到语言（vision2language）的直接对应关系，但忽略了一个重要的事实是，图像和语言之间，其实是存在鸿沟的。
虽然用神经网络将图像空间和语言空间 embed 在同一个空间当中，但这两个空间应该需要一个共同的 sub-space 作为桥梁来连接。于是还有方法提出了attributes，一种图像和语言都拥有的特征。大家可以看左边这张图，基于上面提到的 CNN-RNN 结构，多加了一个 attributes prediction layer。当给定一张图像，先去预测图像当中的各种 attributes（attributes 定义是广义的，包括物体名称，属性，动作，形容词，副词，情绪等等），然后再将这些 attributes 代替之前的 CNN 图像特征，输入到 RNN 当中，生成语句。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk2q7zxt04j310s0kmapw.jpg" alt="image-20201026143736966" style="zoom:50%;" />

#### VQA

VQA是指视觉问答，与其他 vision-to-language 不同的是，当它需要一个机器去回答一个关于图片内容的问题的时候，机器不仅需要能够理解图像以及语言信息，还要能够具有一定的常识。比如，如这张图左边，问题是图中有几只哺乳动物。那么回答这个问题，我们不仅需要机器能够「看」到图中有狗，猫，鸟，还需要机器能够「知道」狗和猫是哺乳动物，而鸟不是，而不只是「告诉」我们正确答案是 2.
									<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026143812273.png" alt="image-20201026143812273" style="zoom:50%;" />



实际上，围绕 image captioning 和 VQA，有很多经典的方法被提出，比如从 machine translation 借鉴来的 sequence-to-sequence model，也就是 cnn-rnn 模型，再到引入 attention（注意力机制），以及以 attributes 作为中间层去生成更准确的 caption 和答案，都是非常经典并且有效的方法。但是我们也发现，尽管方法越来越多，模型越来越复杂，带来的 improvement 却非常有限。

因此在18年左右提出了将多任务框架应用于这个VIsion and Language Grounding Problems，共享下流任务的所有数据，将question和answer组成的pair 以及 对应的图像作为输入去训练回答和生成模型，但是当跨任务数据不一样的时候，这个方法是不work的，另外这会带来数据泄露的问题。

因此，这篇论文提出了将联邦学习的思想应用在Vision and Language Grounding问题，以此在避免数据隐私泄露的前提下，提升任务的性能。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026143952122.png" alt="image-20201026143952122" style="zoom: 33%;" />

### 论文整体思路

这篇论文提出了一种联邦学习框架，可以从不同的任务中获得各种类型的图像表示，然后将它们融合在一起以形成细粒度的图像表示。这些图像表示融合了来自不同视觉和语言的多模态问题的有用图像表示，因此在单个任务中比单独的原始图像表示强大得多。为了学习这种图像表示，该课题组提出了对齐（Aligning），集成（Integrating）和映射（Mapping）网络（aimNet）。aimNet由一个对齐模块，一个集成模块和一个映射模块组成。如下图所示：

![image-20200924094600939](https://i.loli.net/2020/09/24/OnzFZWN4Hj6drok.png)

其中，对齐模块通过对提取的视觉和文本特征进行相互关注来构建对齐的图像表示，其能为显著图像区域提供了更清晰的语义描述。接下来，集成模块着重于通过自我注意机制集成视觉和文本特征，该机制捕获显著区域的分组和属性的搭配。最后，映射模块由两层非线性层组成，用于将学习到的细粒度图像表示映射到特定任务的特征域。各课题组提出的模块充分利用了图像中的所有有效信息，并将其作为输入传递给解码器，以生成有意义的句子或给出问题的准确答案。该课题组在两个图像字幕数据集和一个VQA数据集上，以及相应的三个联邦学习设置上，包括水平联合学习，垂直联合学习和联合迁移学习，进行实验用于验证该课题组的动机以及所提出方法的有效性。

那具体来看，首先每个客户端输入的都是图像，这些图像可以是不同类型的，作者在实验中是用了两个不同的数据集来表示这一点。其次是每个客户端的任务也可以是不同的，比如一部分做视觉问答，另一部分做图像描述。因为他们的任务不一样，所以传统的联邦学习方法就不适用了。作者的方法概括的说就是让服务端去学习一个对应于不同任务的图像表示。具体的可以看上面这张图，论文提出了aimNet框架，可以简单的分为三个阶段，首先第一个阶段客户端在自己的模型上得到一个原始图像表示，也就是一个向量。然后把这个向量发送给服务端。第二个阶段服务端通过聚合不同客户端传来的信息加工得到一个更好的图像表示，再把这个表示发送给客户端，最后第三个阶段客户端就可以用这个图像表示来做自己的任务。

### Visual and Textutal Features

我们看第一个阶段，这个阶段提取输入图像的一个表示，它用到的网络是预训练好的Faster-RCNN，这个网络会生成边界框（bounding boxes）列表，每个边界框的类别标签以及其概率。大家可以看左边这张图，从Fast R-CNN的全连接层中提取到输入图像的视觉特征，这里d表示的是隐藏层神经元的个数，N表示的是候选区域的个数，我这里不展开讲了，感兴趣的同学可以看看更加详细的讲解。
然后提取的另外一种是文本特征，来提供显示的更high level 的图像描述，他这里利用的是multiple instance learning多示例弱监督方法，这个多示例学习是个分类器，大家感兴趣的可以去看一下周志华教授对多示例学习的研究文章。
他的功能是从原来的图像中框出M个候选区域，类似于下面这样，然后他从1000个单词中找出和这个区域最匹配的单词，每个单词都是用d维的词向量表示的。

所以这个阶段是客户端输入一张图像，经过特征提取客户端就得到了两个向量组，一个是表示这个框中图像的I，另一个是表示这些单词的T，即两个序列或者说两个矩阵。每一个客户端都会得到这样的两个矩阵，然后他们发送给服务端，服务端需要对些矩阵进行第二个阶段的加工处理。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026144315949.png" alt="image-20201026144315949" style="zoom:40%;" />

### aimNet

第二个阶段可以分为三个步骤。

#### Aligning Module

首先第一个步骤Aligning Module，对齐模块。这两个向量是我们上一个阶段提取出来的特征，这个I中的每个向量表示了图像的某一个区域，我们需要结合相关语义来获得一个更好的表示，同样的T中的单词需要对应于图像区域来避免歧义的产生。比如mouse这个单词，有可能代表老鼠或者鼠标，需要对应到图像后才知道到底是哪一个。

论文采用了自注意力机制来模拟上述过程，具体来看，首先是MHA的计算，根据这个公式计算得到一个Attention矩阵，推荐大家阅读一下Attention is all you need这篇论文，可以更好地理解注意力机制。

回到论文，这里I就是图像矩阵，T就是文本矩阵，用图像矩阵点积文本矩阵的转置，然后进行缩放，再经过softmax输出后与文本矩阵做点积。
我个人理解的话是，分子中的这个点积操作是为每个向量计算一个score，得到的是两个矩阵之间的相似度，这个相似度就是对于当前模型来说，每一个图像区域对于每一个单词的重要程度。然后为了梯度的稳定，Transformer使用了score归一化，即除以这个分母。最后点积T就是把这个信息融入到了文本特征中。
同样的如果换成下面的TII的话，就是把这个信息融入到了图像特征中。最后，做多次Attention计算，将每一次得到的矩阵进行通道维度的拼接，就是完整的MHA操作。

然后是最外面一层的FFN，这个就比较简单了，他的公式是这样的，就是一个两层的前馈神经网络，中间用了Relu激活函数。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026144412263.png" alt="image-20201026144412263" style="zoom:40%;" />

更加直观的理解这个过程的话我们可以看下面这张图，他要做的就是图像和文本信息的结合，比如，下面这个高亮的人用这些词表示，高亮的狗用这些词来表示。经过这个Aliging Module的步骤，我们得到的还是两个矩阵。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026150533967.png" alt="image-20201026150533967" style="zoom:33%;" />

#### Integrating module

然后我们看下一个步骤，Integrating module，大家可以留意到这一步的过程和上一步比较类似，只不过输入变成了三个一样的矩阵，分别是图像特征和文本特征。那么这个公式是什么意思呢？我们说前一步是结合了图像和文本的信息，那这一步的目标是得到区域之间或者文本之间的一个关系，这也是一个自注意力机制。也就是说这个过程会学习到更加high-level的表征，也就是图像显著特征之间的关系，以及句子单词之间的联系

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026150732300.png" alt="image-20201026150732300" style="zoom: 67%;" />

更直观一点的话我们可以看下面这两张图，输入的图像会标注显著区域，然后各个区域之间的关系将会被计算出来；
假设我输入的是两个一模一样的句子，那最后得到的就是单词之间的一些句法特征或语义特征，比如他会学习到it指代的对象就是前面的animal，以及和它相互依赖的application。这一步的输出也是两个矩阵。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026150932743.png" alt="image-20201026150932743" style="zoom:50%;" />

#### Mapping module

最后Mapping module，是在做一个映射。
之前说的所有的这三步，都是在客户端对图像做一个表征学习，那不同的任务数据空间是不一样的，所以还需要将前面生成的细粒度的图像表征映射到不同的任务上，使得这个框架可以适用于不同的任务。这个公式是一个两层的前馈神经网络，值得注意的是，它是一个分叉的结构，这个分叉的数量取决于客户端所作的任务的种类。也就是说，给视觉问答和图像描述的两个任务的图像表示是不一样的。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026151026127.png" alt="image-20201026151026127" style="zoom:67%;" />

### 下流任务

前面的过程实际上都是在做一个表征学习，输出的是矩阵，但是只有矩阵是不能进行反向传播的，所以需要把这个矩阵回传的客户端，然后让他们用这个矩阵去做各自的一个下游任务，通过这个下游任务来评判得到的这个矩阵的好坏。

所以总的来说，他的整个流程应该是这样的，客户端通过两个预训练好的模型得到图像的两种表示，然后传给服务服务端，服务端加工后得到另外的两种图像表示并传回给客户端，客户端用这两个表示作为输入传入自己的网络做对应的任务，然后每一个客户端根据自己任务的评判指标去更新自己的网络以及服务端的网络，直到他达到一个收敛的状态。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026151114786.png" alt="image-20201026151114786" style="zoom:50%;" />

### 实验

在实验部分，作者首先模拟了联邦学习的三种情况，分别是横向 纵向联邦学习以及联邦迁移。
首先看这个Horizontal federated learning，相当于不同区域的两家银行拥有相同的特征和不同的用户，他们的业务是相似的。每个客户端使用不同的数据集做相同同的任务，都是在做生成图像描述的任务。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026151443057.png" alt="image-20201026151443057" style="zoom:50%;" />

那这个Vertical FL，每个客户端使用相同的数据集，完成不同的任务，相当于同一区域的银行和金融公司，由于包含这个区域的绝大多数居民，所以他们的用户交集是很大的，但由于业务性质不同，他们用户的特征很不一样。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk2rb1j26pj31320lsn40.jpg" alt="image-20201026151506885" style="zoom:50%;" />

那在federated  transfer learning中，不同客户端使用不同的数据集做不同的任务。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk2rd0b3rsj31000lu0zy.jpg" alt="image-20201026151642641" style="zoom:50%;" />

我们可以看到对于baseline来说的话不管是什么数据集都是作者提出的这种要好。三种方法都是如此，这就说明作者提出的这个方法是确实有效的，这是他的一个总体情况。

### 单模块的分析

此外作者还对模型的每一部分做了分析。分别用Aligning和Integrating去做图像描述和视觉问答，以这张图为例，在做图像描述的时候，Aligning会注意到这是个红色的杯子以及桌子的材质，而Integrating则是注意到了与杯子相邻的菠萝。更通用的说，Aligning偏向于物体的颜色或者某种属性，这个是物体本身固有的，而Integrating更偏向于和当前物体有关的其他的一些物体。同样的，在视觉问答中，Aligning偏向于回答在哪里或者是什么这种问题，而Integrating对物体的数量特别的敏感

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk2rdzuaayj313k0luqfh.jpg" alt="image-20201026151756535" style="zoom:50%;" />

这和我们之前的分析是一致的，Aligning得到的是图像和文本间的关系，也就是图中红色的框对应蛋糕，红色，而蓝色的框对应胡萝卜、蔬菜。而Integrating则是会从蛋糕出发，找到一些和他相关的区域，或者是一些和他相关的单词。所以Aligning对视觉问答的提升较大而Integrating对图像描述的提升会较大，另外同时使用这两个会在不同的任务上得到更好的效果。

<img src="/Users/cherry/Library/Application Support/typora-user-images/image-20201026151816392.png" alt="image-20201026151816392" style="zoom:50%;" />

### 一些个人看法

这篇文章在我看来其实是提出一个aimNet表征学习框架，将他适配到联邦学习中，并且共享的数据实际上还是数据本身，只是经过了表征学习转换而已。服务端获得的是所有图像的两种表示（经过表征学习），每个客户端本地的模型是固定的，如果这个模型被获取到，或者是通过反向计算得到输入的图像，就会造成客户端数据的泄露。