---
layout:     post
title:      FedMD: Heterogenous Federated Learning via Model Distillation
subtitle:   FedMDå°†æ¨¡å‹è’¸é¦è¿ç”¨äºè”é‚¦å­¦ä¹ ï¼Œä¼ é€’çš„æ˜¯soft scoreï¼Œè€Œä¸æ˜¯æ¢¯åº¦
date:       2020-08-07
author:     CY
header-img: img/post-bg-food3.jpg
catalog: 	 true
tags:
    - Federated Learning
    - Knowledgeable Distillation
    - Deep Learning
	- è®ºæ–‡
---

[**è®ºæ–‡é“¾æ¥**ğŸ”—](https://arxiv.org/abs/1910.03581)

[**ä½œè€…è¯¦è§£**ğŸ”—](https://www.bilibili.com/video/av84088104/)

**å›é¡¾ä¸€ä¸‹FedAvgç®—æ³•æµç¨‹ï¼š**

1. æ¯ä¸€è½®è¿­ä»£å¼€å§‹å‰ï¼Œserveréšæœºé€‰å–éƒ¨åˆ†clients - $max(CÂ·K,1)$ ï¼Œå°†å½“å‰çš„å…¨å±€æ¨¡å‹å‚æ•°åˆ†å‘ç»™é€‰ä¸­çš„å®¢æˆ·ç«¯
2. clientåšå‚æ•°åˆå§‹åŒ–ï¼Œåœ¨æœ¬åœ°æ‰§è¡Œ$E$ä¸ª*epoch*çš„æ›´æ–°åï¼Œå°†æ›´æ–°åçš„å‚æ•°ä¸Šä¼ ç»™serverï¼Œå³ä¸‹å›¾çš„*ClientUpdate*ç®—æ³•
3. serveræŒ‰ç…§ä¸‹å›¾çº¢è‰²æ¡†çš„æ–¹æ³•è®¡ç®—å‚æ•°çš„åŠ æƒå¹³å‡å¹¶æ›´æ–°
4. é‡å¤æ­¥éª¤1-3

> FedAvgç®—æ³•åœ¨æœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯ä¼ é€’çš„æ˜¯æ•´ä¸ªæ¨¡å‹çš„å‚æ•°ï¼ˆæ¢¯åº¦ï¼‰ï¼Œå½“æ¨¡å‹è§„æ¨¡è¾ƒå¤§æ—¶ä¼ é€’çš„å‚æ•°ä¹Ÿä¼šå¾ˆå¤šï¼Œé€šä¿¡æˆæœ¬ä¹Ÿä¼šæ¿€å¢ã€‚

<img src="https://i.loli.net/2020/08/07/GEIlxWTQkLJeVz7.png" style="zoom: 50%;" />

#### FedMD 

##### æ€è·¯



- Communication at a high levelâ€”â€”simplest approach: **model distillation**

- Key components: model distillation and **transfer learning**ï¼ˆå¼•å…¥å…¬å…±æ•°æ®é›†ï¼‰

##### ç®—æ³•æµç¨‹

![image-20200808190039493](https://i.loli.net/2020/08/08/bogR7OY2MX5DiPV.png)

>We re-purpose the public dataset $D_0$ as the basis of communication between models, which is realized using knowledge distillation. Each learner $f_k$ expresses its knowledge by sharing the class scores, $f_k(x_i^0)$, computed on the public dataset $D_0$. The central server collects these class scores and computes an average $\widetilde{f}(x_i^0)$. Each party then trains $f_k$ to approach the consensus $\widetilde{f}(x_i^0)$. In this way, the knowledge of one participant can be understood by others without explicitly
> sharing its private data or model architecture. Using the entire large public dataset can cause a large communication burden. In practice, the server may randomly select a much smaller subset $d_jâŠ‚D_0$ at each round as the basis of communication. In this way, the cost is under control and does not scale with the complexity of participating models.

##### å®éªŒç»“æœ

è®ºæ–‡åšäº†ä¸¤ç»„å®éªŒï¼ˆå‡è€ƒè™‘äº†iid å’Œ Non-iidæƒ…å†µï¼‰

- Hand written digits and character
  - Public datasets = MNISTï¼ˆæ‰‹å†™æ•°å­—ï¼‰
  - Private datasets = subset of FEMNISTï¼ˆæ‰‹å†™å­—æ¯ï¼‰
- Colored images of pets and objects
  - Public datasets = CIFAR10
  - Private datasets = subset of CIFAR100

clienté‡‡ç”¨çš„æ˜¯2-4å±‚çš„CNNï¼Œä½¿ç”¨ä¸åŒçš„é€šé“æ•°å’Œdropoutã€‚å›¾ä¸­çš„æŠ˜çº¿è¡¨ç¤ºFedMDç®—æ³•æ¯ä¸ªclientåœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚å·¦ä¸‹æ–¹çš„è™šçº¿æ˜¯æŒ‡ä¸ä½¿ç”¨è”é‚¦å­¦ä¹ ï¼ˆåœ¨transfer learningä¹‹å collaborationä¹‹å‰ï¼‰çš„performanceï¼Œclientåªåœ¨è‡ªå·±çš„ç§æœ‰æ•°æ®ä»¥åŠå…¬æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç»“æœï¼Œè¿™æ˜¯<u>baseline</u>ã€‚å³ä¸Šè§’çš„è™šçº¿è¡¨ç¤ºæ¯ä¸ªclientæ‹¥æœ‰å…¨éƒ¨çš„æ•°æ®æ‰€èƒ½è¾¾åˆ°çš„å®éªŒç»“æœã€‚

ç»“æœè¡¨ç¤ºï¼ŒFedMDç®—æ³•è¾¾åˆ°äº†æ¯”baselineæ›´å¥½çš„æ€§èƒ½ï¼Œä½†æ˜¯æ²¡æ³•è¾¾åˆ°æœ€ä¼˜ç»“æœã€‚

>First they are trained on the public dataset until convergence, â€” these models typically have test accuracy around 99% on MNIST and 76% on CIFAR10. Secondly each participant trains its model on its own small private dataset. After these steps, they go through the collaborative training phase, during which the models acquire strong and fast improvements across the board, and quickly outperform the baseline of transfer learning.

<img src="https://i.loli.net/2020/08/08/ySHrEOb5Cd64fVX.png" alt="image-20200808194951033" style="zoom:67%;" />