---
layout:     post
title:      Federated Learning for Vision-and-Language Grounding Problems
subtitle:  多模态机器学习旨在通过机器学习的方法实现处理和理解多源模态信息的能力。这篇论文提出了一种联邦学习框架，可以从不同的任务中获得各种类型的图像表示，然后将它们融合在一起以形成细粒度的图像表示
date:       2020-09-27
author:     CY
header-img: img/post-bg-food3.jpg
catalog: 	 true
tags:
    - Federated Learning
    - Multimodal
---


### 前述

这篇论文提出了一种联邦学习框架，可以从不同的任务中获得各种类型的图像表示，然后将它们融合在一起以形成细粒度的图像表示。这些图像表示融合了来自不同视觉和语言的多模态问题的有用图像表示，因此在单个任务中比单独的原始图像表示强大得多。为了学习这种图像表示，该课题组提出了对齐（Aligning），集成（Integrating）和映射（Mapping）网络（aimNet）。aimNet由一个对齐模块，一个集成模块和一个映射模块组成。如下图所示：

![image-20200924094600939](https://i.loli.net/2020/09/24/OnzFZWN4Hj6drok.png)

其中，对齐模块通过对提取的视觉和文本特征进行相互关注来构建对齐的图像表示，其能为显著图像区域提供了更清晰的语义描述。接下来，集成模块着重于通过自我注意机制集成视觉和文本特征，该机制捕获显著区域的分组和属性的搭配。最后，映射模块由两层非线性层组成，用于将学习到的细粒度图像表示映射到特定任务的特征域。各课题组提出的模块充分利用了图像中的所有有效信息，并将其作为输入传递给解码器，以生成有意义的句子或给出问题的准确答案。该课题组在两个图像字幕数据集和一个VQA数据集上，以及相应的三个联邦学习设置上，包括水平联合学习，垂直联合学习和联合迁移学习，进行实验用于验证该课题组的动机以及所提出方法的有效性。

### 文章主要工作：

- 提出一种联邦学习框架，通过生成细粒度的图像表示，框架提高了在不需要共享下游任务数据的情况下vision-and-language grounding问题的性能
- 设计对齐、集成和映射网络【Aligning, Integrating and Mapping Net work (aimNet)】，在框架中实现了分布式模型，有效 自动地并将从图像提取出的视觉和文本特征转换成细粒度图像表征。
- 在三个联邦学习设置上进行了验证

### 方法：

>  1) The visual and textual features extractor; 2) The designed centralized model - aimNet and 3) The implementations in three federated learning settings. We first introduce the visual and textual feature extractors. We then discuss the aligning, integrating and mapping network (aimNet) in detail (see Figure 1). Finally, we describe the three federated learning settings

#### Visual and Textual Feature

利用了从Faster R-CNN中提取的基于RCNN的特征，采用多实例学习的弱监督方法来构建语义概念提取器。

抽取的图像特征：

<img src="https://i.loli.net/2020/09/24/7xATVlhzpGqUiJ5.png" alt="image-20200924111115702" style="zoom: 67%;" /> 

文本特征：  the word embeddings for a list of semantic concepts

<img src="C:\Users\陈玥\AppData\Roaming\Typora\typora-user-images\image-20200924151958993.png" alt="image-20200924151958993" style="zoom:67%;" /> 

> The concepts could be objects (e.g. *dog*, *frisbee*), attributes (e.g. *off*, *electric*), or relationships (e.g. *holding*, *flying*). The textual features represents the image from a semantic perspective, providing a powerful bias for vision-and-language tasks.

#### Aligning, Integrating and Mapping Network

##### Basic Module

为了提取视觉特征与文本特征的内部模态和内部模态之间的关系，采用了**Multi-Head Attention(MHA)**和**Feed-Forward Network(FFN)**，该算法计算不同特征之间的关联权重。利用MHA实现了基于语义表示的视觉和文字特征对齐。

##### Aligning Module

为了更有意义地表达图像特征，我们需要从文本特征中找到最相关的语义概念来概括图像特征，同样我们也需要提供图像表示给文本特征来减少语义冲突（比如`mouse`可以表示老鼠或者鼠标），根据注意力定理，我们可以通过以下公式模拟以上情况

<img src="https://i.loli.net/2020/09/24/Y7WPHExpkeRm6XT.png" alt="image-20200924105501087" style="zoom:80%;" /> 

##### Integrating Module

描述图像的时候，我们通常专注于某个特定区域，并寻找经常出现在该区域附近的其他区域。基于此，集成模块应该从特定分类中学习那些与空间或语义相关的对象，因此，每个在图像特征$I$中的特定空间向量（region vector） 和 文本特征$T$中的特定概念向量（ concept vector）应该分别专注在整个图像特征$I$和文本特征$T$，去找到最相关的图像区域和属性组合，生成基于方面的图像表征，这个过程可以定义如下：

<img src="C:\Users\陈玥\AppData\Roaming\Typora\typora-user-images\image-20200924143916056.png" alt="image-20200924143916056" style="zoom:67%;" /> 

通过该公式，在可视域中，集成模块可以学习显着区域分组（salient region groupings），并集成相关的图像区域，以进行更高层次的图像表示。在文本领域，它学习属性搭配（ attribute collocations），并具有在句子表达过程中考虑关联和搭配的能力。所获取的基于方面的图像表示对于图像字幕任务超级有好处

##### Mapping Module

不同任务有不同的数据空间，所以我们应该将细粒度图像表征映射到任务空间，让提出的框架可以适用于不同的任务，向下游任务注入了丰富的新。因此，我们引入了mapping module，定义如下：

<img src="C:\Users\陈玥\AppData\Roaming\Typora\typora-user-images\image-20200924151623148.png" alt="image-20200924151623148" style="zoom:67%;" /> 

#### Implementation

##### Horizontal Federated Learning（横向联邦学习） sample-based

> 数据的特征空间是一样的，比如不同城市的两个银行，有不同的用户users，但因为他们有相同的业务business因此他们的features spaces可能是一样的

在论文的应用场景中，将**两种不同的图像描述**数据集当作两个银行（相同的business都是 <u>生成描述</u>，不同的users是输入的图像）

<img src="https://i.loli.net/2020/09/24/FiTZpPKfnGVlWIc.png" alt="image-20200924153228492" style="zoom: 67%;" /> 

先在task 1上运行一个实例，然后根据训练损失更新task 1 的 task-specific decoder和aimNet；然后在task 2上运行实例，然后同上更新。aimNet可以从不同任务中获得多种类型的图像表征，然后可以比在单个的任务中更好地学习细粒度图像表征，并且不同任务的源信息间没有共享。在训练和推理过程中，每个任务的运行都是独立的。

##### Vertical Federated Learning（纵向联邦学习） feature-based

> 两个数据集的users是一样的，特征空间不同，比如在同一城市的两个不同公司，银行和保险公司，他们的users集合可能包含了该地区的大多数居民，因此用户域的重合可能很大，但由于两个公司的不同业务，他们的特征域差距会很大。

应用到当前情况，我们将两个不同的下游任务看作是两家不同的公司，共享一样的users(input images)。使用这两个数据集是因为大部分VQA数据集来自于MSCOCO

<img src="C:\Users\陈玥\AppData\Roaming\Typora\typora-user-images\image-20200924164029098.png" alt="image-20200924164029098" style="zoom:67%;" /> 

##### Federated Transfer Learning

因为两个数据集samples和feature space都不相同

<img src="https://i.loli.net/2020/09/24/mbiQnLfw2RAzX1Z.png" alt="image-20200924174858369" style="zoom:67%;" /> 